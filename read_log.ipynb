{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6d64f12-01a9-4f0f-806e-b91fe064f95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import glob\n",
    "from typing import Dict, List\n",
    "\n",
    "def parse_log_to_json(log_content: str) -> Dict:\n",
    "    # 初始化结果字典\n",
    "    result = {\n",
    "        \"experiment_config\": {},\n",
    "        \"training_metrics\": []\n",
    "    }\n",
    "\n",
    "    # 解析实验参数配置\n",
    "    config_start = False\n",
    "    for line in log_content.split('\\n'):\n",
    "        if \"实验参数配置:\" in line:\n",
    "            config_start = True\n",
    "            continue\n",
    "        if config_start and \"===\" in line:\n",
    "            config_start = False\n",
    "            continue\n",
    "        if config_start and \"INFO - \" in line:\n",
    "            parts = line.split(\"INFO - \")\n",
    "            if len(parts) > 1:\n",
    "                parts = parts[1].split(\": \", 1)\n",
    "                if len(parts) == 2:\n",
    "                    key, value = parts[0], parts[1]\n",
    "                    # 类型转换\n",
    "                    if value == \"True\":\n",
    "                        value = True\n",
    "                    elif value == \"False\":\n",
    "                        value = False\n",
    "                    elif value.startswith(\"[\") and value.endswith(\"]\"):\n",
    "                        value = [item.strip(\"'\\\" \") for item in value[1:-1].split(\",\")]\n",
    "                    elif \".\" in value and value.replace(\".\", \"\", 1).isdigit():\n",
    "                        value = float(value)\n",
    "                    else:\n",
    "                        try:\n",
    "                            value = int(value)\n",
    "                        except:\n",
    "                            pass\n",
    "                    result[\"experiment_config\"][key] = value\n",
    "\n",
    "    # 解析训练指标\n",
    "    epoch_pattern = re.compile(r\"INFO - Epoch (\\d+)/5\")\n",
    "    \n",
    "    # 时间指标模式\n",
    "    data_transfer_pattern = re.compile(r\"INFO -   数据传输总耗时: ([\\d\\.]+)秒\")\n",
    "    forward_pattern = re.compile(r\"INFO -   前向传播总耗时: ([\\d\\.]+)秒\")\n",
    "    backward_pattern = re.compile(r\"INFO -   反向传播总耗时: ([\\d\\.]+)秒\")\n",
    "    optimizer_pattern = re.compile(r\"INFO -   优化器步骤总耗时: ([\\d\\.]+)秒\")\n",
    "    epoch_duration_pattern = re.compile(r\"INFO - Epoch \\d+ 完成，耗时 ([\\d\\.]+) 秒。\")\n",
    "    \n",
    "    # 评估指标模式\n",
    "    eval_duration_pattern = re.compile(r\"INFO - 评估完成，耗时 ([\\d\\.]+) 秒。\")\n",
    "    eval_result_pattern = re.compile(r\"INFO - 评估结果: \\{'pearson': ([\\d\\.]+)\\}\")\n",
    "    \n",
    "    lines = log_content.split('\\n')\n",
    "    current_epoch = None\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        # 解析epoch开始\n",
    "        epoch_match = epoch_pattern.search(line)\n",
    "        if epoch_match:\n",
    "            current_epoch = int(epoch_match.group(1))\n",
    "            result[\"training_metrics\"].append({\n",
    "                \"epoch\": current_epoch,\n",
    "                \"time_metrics\": {},\n",
    "                \"evaluation\": {}\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # 已有epoch的情况下解析指标\n",
    "        if current_epoch is not None:\n",
    "            current_metrics = next((m for m in result[\"training_metrics\"] if m[\"epoch\"] == current_epoch), None)\n",
    "            \n",
    "            if current_metrics:\n",
    "                # 解析时间指标\n",
    "                data_transfer_match = data_transfer_pattern.search(line)\n",
    "                if data_transfer_match:\n",
    "                    current_metrics[\"time_metrics\"][\"data_transfer\"] = float(data_transfer_match.group(1))\n",
    "                    continue\n",
    "                \n",
    "                forward_match = forward_pattern.search(line)\n",
    "                if forward_match:\n",
    "                    current_metrics[\"time_metrics\"][\"forward\"] = float(forward_match.group(1))\n",
    "                    continue\n",
    "                \n",
    "                backward_match = backward_pattern.search(line)\n",
    "                if backward_match:\n",
    "                    current_metrics[\"time_metrics\"][\"backward\"] = float(backward_match.group(1))\n",
    "                    continue\n",
    "                \n",
    "                optimizer_match = optimizer_pattern.search(line)\n",
    "                if optimizer_match:\n",
    "                    current_metrics[\"time_metrics\"][\"optimizer_step\"] = float(optimizer_match.group(1))\n",
    "                    continue\n",
    "                \n",
    "                duration_match = epoch_duration_pattern.search(line)\n",
    "                if duration_match:\n",
    "                    current_metrics[\"time_metrics\"][\"epoch_duration\"] = float(duration_match.group(1))\n",
    "                    continue\n",
    "                \n",
    "                # 解析评估指标\n",
    "                eval_duration_match = eval_duration_pattern.search(line)\n",
    "                if eval_duration_match:\n",
    "                    current_metrics[\"evaluation\"][\"duration\"] = float(eval_duration_match.group(1))\n",
    "                    continue\n",
    "                \n",
    "                eval_result_match = eval_result_pattern.search(line)\n",
    "                if eval_result_match:\n",
    "                    current_metrics[\"evaluation\"][\"pearson\"] = float(eval_result_match.group(1))\n",
    "                    continue\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53be4ec0-64c7-4018-b074-17b35026e9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_files(folder_path: str, output_file: str) -> None:\n",
    "    \"\"\"\n",
    "    处理指定文件夹下的所有.txt和.log文件，解析为JSON并保存到单个文件\n",
    "    \n",
    "    Args:\n",
    "        folder_path: 包含日志文件的文件夹路径\n",
    "        output_file: 输出JSON文件的路径\n",
    "    \"\"\"\n",
    "    # 获取所有的.txt和.log文件\n",
    "    file_paths = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
    "    file_paths.extend(glob.glob(os.path.join(folder_path, \"*.log\")))\n",
    "    \n",
    "    # 存储所有解析结果的数组\n",
    "    all_results = []\n",
    "    \n",
    "    # 处理每个文件\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                log_content = f.read()\n",
    "            \n",
    "            # 添加文件名信息到日志内容开头\n",
    "            file_name = os.path.basename(file_path)\n",
    "            log_content = f\"{file_name}\\n{log_content}\"\n",
    "            \n",
    "            # 解析日志\n",
    "            result = parse_log_to_json(log_content)\n",
    "            \n",
    "            # 如果解析成功，添加文件路径信息\n",
    "            if result and \"experiment_config\" in result:\n",
    "                result[\"file_path\"] = file_path\n",
    "                all_results.append(result)\n",
    "                print(f\"成功解析 {file_path}\")\n",
    "            else:\n",
    "                print(f\"解析 {file_path} 失败，未找到有效的实验配置\")\n",
    "        except Exception as e:\n",
    "            print(f\"处理文件 {file_path} 时出错: {str(e)}\")\n",
    "    \n",
    "    # 将所有结果保存到一个JSON文件\n",
    "    if all_results:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"成功将 {len(all_results)} 个实验结果保存到 {output_file}\")\n",
    "    else:\n",
    "        print(f\"没有找到可解析的文件，未生成输出\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb323763-5e93-4cd4-8eba-179cdb5e52f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功解析 ./data/stsb_lora_rank3\\stsb_lora_rank3_20250323_122038.txt\n",
      "成功解析 ./data/stsb_lora_rank3\\stsb_lora_rank3_20250323_123635.txt\n",
      "成功解析 ./data/stsb_lora_rank3\\stsb_lora_rank3_20250323_125253.txt\n",
      "成功解析 ./data/stsb_lora_rank3\\stsb_lora_rank3_20250323_130904.txt\n",
      "成功解析 ./data/stsb_lora_rank3\\stsb_lora_rank3_seed42.txt\n",
      "成功将 5 个实验结果保存到 ./data/stsb_lora_rank3.json\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"./data/stsb_lora_rank3\"\n",
    "output_file = \"./data/stsb_lora_rank3.json\"\n",
    "\n",
    "process_log_files(folder_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "660ba84d-7579-4670-b863-8ca9ba713bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功解析 ./data/stsb_adapter\\stsb_adapter_rank_20250323_154600.log\n",
      "成功解析 ./data/stsb_adapter\\stsb_adapter_rank_20250323_160057.log\n",
      "成功解析 ./data/stsb_adapter\\stsb_adapter_rank_20250323_161607.log\n",
      "成功解析 ./data/stsb_adapter\\stsb_adapter_rank_20250323_163105.log\n",
      "成功解析 ./data/stsb_adapter\\stsb_adapter_rank_20250323_164602.log\n",
      "成功将 5 个实验结果保存到 ./data/stsb_adapter.json\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"./data/stsb_adapter\"\n",
    "output_file = \"./data/stsb_adapter.json\"\n",
    "\n",
    "process_log_files(folder_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8fcf075-04e1-446c-9a8b-36e842e23ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功解析 ./data/ag_news_lora\\ag_news_lora_rank3_20250324_001933.log\n",
      "成功将 1 个实验结果保存到 ./data/ag_news_lora.json\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"./data/ag_news_lora\"\n",
    "output_file = \"./data/ag_news_lora.json\"\n",
    "\n",
    "process_log_files(folder_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c444ad2-094e-4f40-b34d-26e9da034817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功解析 ./data/ag_news_adapter\\ag_news_adapter_rank_20250323_002101.log\n",
      "成功将 1 个实验结果保存到 ./data/ag_news_adapter.json\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"./data/ag_news_adapter\"\n",
    "output_file = \"./data/ag_news_adapter.json\"\n",
    "\n",
    "process_log_files(folder_path, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora",
   "language": "python",
   "name": "lora"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
